{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 1 - Dominik Wiśniewski"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The description of the task shows that the task consists of classification into three different classes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "My idea for this task looks like this: I will first check the data for duplicates, outliers and missing values in the records. Then, without any processing of my data, I will teach an artificial neural network and evaluate its effectiveness. Then I will use simpler models to speed up learning, after assessing the models I will process the data and try to improve all the metrics used in this task (precision, recall, and f1 score)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import tensorflow as tf\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import make_scorer, accuracy_score, f1_score, recall_score, precision_score\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import PassiveAggressiveClassifier\n",
    "from sklearn.linear_model.ridge import RidgeClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load file\n",
    "dataset = pd.read_csv('Graduate - IRISES dataset (2019-06).csv', sep=\"|\")\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(dataset))\n",
    "dataset.drop_duplicates(keep=False,inplace=True)\n",
    "print(len(dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f,ax = plt.subplots(figsize=(10,10))\n",
    "sns.heatmap(dataset.corr(), annot = True,linewidths=.4, fmt='.1f', ax=ax)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset['Species'].value_counts().plot(kind='bar')\n",
    "plt.title(f\"Zrównoważenie klas\")\n",
    "plt.xlabel(f\"Species\")\n",
    "plt.ylabel(\"Ilość\")\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.pairplot(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset had one empty element that was not significant against background 149, so instead of filling in the missing data with the median or average of the remaining values, the record was simply deleted. There were also two duplicates that were removed. One of the records had a comma instead of a dot, but the record was also only one so instead of using the replace () method I corrected it manually. There is a large correlation between some of the features, which can slow down and hinder the learning of some models. Classes are balanced, so no downsampling or upsampling."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next step will be learning the machine learning model. The first model will be an artificial neural network due to its high ability to generalize and deal with correlated data. In most problems related to function approximation, one hidden layer is sufficient to approximate discrete labels (Basheer and Hajmeer, 2000)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helping mapping function to make one hot encoding\n",
    "def map_labels(labels: np.array) -> list:\n",
    "        \"\"\"\n",
    "        Mapping iris data labels to categorical values\n",
    "        :param labels: numpy.Arrays contains labels\n",
    "        :return: list of mapped values\n",
    "        \"\"\"\n",
    "        mapped = [\n",
    "            np.array([1, 0, 0]) if x == 'setosa' else np.array([0, 1, 0]) if x == 'versicolor' else np.array(\n",
    "                [0, 0, 1]) for x in labels]\n",
    "        return mapped\n",
    "\n",
    "seed = 1234\n",
    "test_size = 0.5\n",
    "    \n",
    "# preprocessing data\n",
    "train_array = dataset.values\n",
    "np.random.shuffle(train_array)\n",
    "\n",
    "X = train_array[:, 0:4].astype(float)\n",
    "Y = train_array[:, 4]\n",
    "\n",
    "Y = np.array(map_labels(Y)).astype(float)\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=test_size, random_state=seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining model\n",
    "model = tf.keras.models.Sequential()\n",
    "model.add(tf.keras.layers.Dense(8, activation=tf.nn.relu))\n",
    "model.add(tf.keras.layers.Dense(3, activation=tf.nn.softmax))\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'neural_network_model'\n",
    "if os.path.exists(model_name):\n",
    "    model = tf.keras.models.load_model(model_name)\n",
    "else:\n",
    "    model.fit(X_train, Y_train, batch_size=8, epochs=100, workers=4, use_multiprocessing=True, verbose=0)\n",
    "\n",
    "val_loss, val_acc = model.evaluate(X_test, Y_test, verbose=0)\n",
    "\n",
    "print(f\"Accuracy: {val_acc}, loss: {val_loss}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The neural network model achieved very high results when split the set into training and test in the 50% -50% ratio, and it did not require further work on the data. But neural network models sometimes consume many times higher resources than simpler models, and it's worth checking how other models will do. At first with the default parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_labels(labels):\n",
    "        \"\"\"\n",
    "        Maping iris data labels to numeric\n",
    "        :param labels: numpy.Arrays contains labels\n",
    "        :return: list of mapped values\n",
    "        \"\"\"\n",
    "        maped = [0.0 if x == 'setosa' else 1.0 if x == 'versicolor' else 2.0 for x in labels]\n",
    "        return maped\n",
    "\n",
    "seed = 1234\n",
    "test_size = 0.5\n",
    "    \n",
    "# preprocessing data\n",
    "train_array = dataset.values\n",
    "np.random.shuffle(train_array)\n",
    "\n",
    "X = train_array[:, 0:4].astype(float)\n",
    "Y = train_array[:, 4]\n",
    "\n",
    "Y = np.array(map_labels(Y)).astype(float)\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=test_size, random_state=seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = [LogisticRegression(),\n",
    "         DecisionTreeClassifier(),\n",
    "         KNeighborsClassifier(),\n",
    "         GaussianNB(),\n",
    "         RandomForestClassifier(), \n",
    "         GradientBoostingClassifier(),\n",
    "         SVC()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TestModels = pd.DataFrame()\n",
    "tmp = {}\n",
    "\n",
    "for model in models:\n",
    "    model_obj = str(model)\n",
    "    model_name = model_obj[:model_obj.index('(')]\n",
    "    tmp['Model'] = model_name\n",
    "    \n",
    "    print(f\"Calculating {model_name} model\")\n",
    "    \n",
    "    print(\"Start training model...\")\n",
    "    if os.path.exists('Default/{}_model.npy'.format(model_name)):\n",
    "        try:\n",
    "            model = np.load('Default/{}_model.npy'.format(model_name), allow_pickle=True).item()\n",
    "        except ValueError:\n",
    "            model.fit(X_train, Y_train)\n",
    "            np.save(f\"Default/{model_name}_model.npy\", model)\n",
    "            \n",
    "    else:\n",
    "        model.fit(X_train, Y_train)\n",
    "        np.save(f\"Default/{model_name}_model.npy\", model)\n",
    "    \n",
    "    print(\"Calculating scores....\")\n",
    "    tmp['Accuracy'] = accuracy_score(Y_test, model.predict(X_test))\n",
    "    tmp['Precision'] = precision_score(Y_test, model.predict(X_test), average='weighted')\n",
    "    tmp['Recall'] = recall_score(Y_test, model.predict(X_test), average='weighted')\n",
    "    tmp['F1_Score'] = f1_score(Y_test, model.predict(X_test), average='weighted')\n",
    "    \n",
    "    print(f\"Accuracy: {tmp['Accuracy']}\\nPrecision: {tmp['Precision']}\\nRecall: {tmp['Recall']}\\nF1 Score: {tmp['F1_Score']}\")\n",
    "    \n",
    "    TestModels = TestModels.append([tmp])\n",
    "\n",
    "TestModels.set_index('Model', inplace=True)\n",
    "fig, axes = plt.subplots(ncols=1, figsize=(10, 4))\n",
    "TestModels.Accuracy.plot(ax=axes, kind='bar', title='Accuracy')\n",
    "plt.show()\n",
    "\n",
    "#TestModels.set_index('Model', inplace=True)\n",
    "fig, axes = plt.subplots(ncols=1, figsize=(10, 4))\n",
    "TestModels.Precision.plot(ax=axes, kind='bar', title='Precision')\n",
    "plt.show()\n",
    "\n",
    "#TestModels.set_index('Model', inplace=True)\n",
    "fig, axes = plt.subplots(ncols=1, figsize=(10, 4))\n",
    "TestModels.Recall.plot(ax=axes, kind='bar', title='Recall')\n",
    "plt.show()\n",
    "\n",
    "#TestModels.set_index('Model', inplace=True)\n",
    "fig, axes = plt.subplots(ncols=1, figsize=(10, 4))\n",
    "TestModels.F1_Score.plot(ax=axes, kind='bar', title='F1_Score')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Simpler machine learning models have also done well according to each of the metrics used. However, their effectiveness can be further improved. I will start by reducing dimensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_model = PCA(n_components=2)\n",
    "pca_model.fit(X)\n",
    "X_2D = pca_model.transform(X)\n",
    "X2D_train, X2D_test, Y_train, Y_test = train_test_split(X_2D, Y, test_size=0.5, random_state=seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TestModels = pd.DataFrame()\n",
    "tmp = {}\n",
    "\n",
    "for model in models:\n",
    "    model_obj = str(model)\n",
    "    model_name = model_obj[:model_obj.index('(')]\n",
    "    tmp['Model'] = model_name\n",
    "    \n",
    "    print(f\"Calculating {model_name} model\")\n",
    "    \n",
    "    print(\"Start training model...\")\n",
    "    if os.path.exists('DefPCA/{}_model.npy'.format(model_name)):\n",
    "        try:\n",
    "            model = np.load('DefPCA/{}_model.npy'.format(model_name), allow_pickle=True).item()\n",
    "        except ValueError:\n",
    "            model.fit(X2D_train, Y_train)\n",
    "            np.save(f\"DefPCA/{model_name}_model.npy\", model)\n",
    "            \n",
    "    else:\n",
    "        model.fit(X2D_train, Y_train)\n",
    "        np.save(f\"DefPCA/{model_name}_model.npy\", model)\n",
    "    \n",
    "    print(\"Calculating scores....\")\n",
    "    tmp['Accuracy'] = accuracy_score(Y_test, model.predict(X2D_test))\n",
    "    tmp['Precision'] = precision_score(Y_test, model.predict(X2D_test), average='weighted')\n",
    "    tmp['Recall'] = recall_score(Y_test, model.predict(X2D_test), average='weighted')\n",
    "    tmp['F1_Score'] = f1_score(Y_test, model.predict(X2D_test), average='weighted')\n",
    "    \n",
    "    print(f\"Accuracy: {tmp['Accuracy']}\\nPrecision: {tmp['Precision']}\\nRecall: {tmp['Recall']}\\nF1 Score: {tmp['F1_Score']}\")\n",
    "    \n",
    "    TestModels = TestModels.append([tmp])\n",
    "\n",
    "TestModels.set_index('Model', inplace=True)\n",
    "fig, axes = plt.subplots(ncols=1, figsize=(10, 4))\n",
    "TestModels.Accuracy.plot(ax=axes, kind='bar', title='Accuracy')\n",
    "plt.show()\n",
    "\n",
    "#TestModels.set_index('Model', inplace=True)\n",
    "fig, axes = plt.subplots(ncols=1, figsize=(10, 4))\n",
    "TestModels.Precision.plot(ax=axes, kind='bar', title='Precision')\n",
    "plt.show()\n",
    "\n",
    "#TestModels.set_index('Model', inplace=True)\n",
    "fig, axes = plt.subplots(ncols=1, figsize=(10, 4))\n",
    "TestModels.Recall.plot(ax=axes, kind='bar', title='Recall')\n",
    "plt.show()\n",
    "\n",
    "#TestModels.set_index('Model', inplace=True)\n",
    "fig, axes = plt.subplots(ncols=1, figsize=(10, 4))\n",
    "TestModels.F1_Score.plot(ax=axes, kind='bar', title='F1_Score')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reducing dimensions to 2 has accelerated learning that has minimized the effectiveness of some models at the same time. Increasing their effectiveness is even more possible by optimizing the hyperparameters of individual models or scaling data, for example, to a scale of 0-1. I decided to limit myself to automating the process of optimizing hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models_with_parameters = [\n",
    "\n",
    "    {\n",
    "        'model_name': 'Logistic Regression',\n",
    "        'model': LogisticRegression,\n",
    "        'params': {\n",
    "            'C': [1, 3, 5, 7, 9],\n",
    "            'solver': ('newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga'),\n",
    "            'n_jobs': [4],\n",
    "        }\n",
    "    }, \n",
    "    {\n",
    "        'model_name': 'Decision Tree',\n",
    "        'model': DecisionTreeClassifier,\n",
    "        'params': {\n",
    "            'max_depth': np.arange(2, 4),\n",
    "        }\n",
    "    }, \n",
    "    {\n",
    "        'model_name': 'KNN',\n",
    "        'model': KNeighborsClassifier,\n",
    "        'params': {\n",
    "            'n_neighbors': [5, 10, 15, 20, 25, 30],\n",
    "            'weights': ('uniform', 'distance'),\n",
    "            'algorithm': ('ball_tree', 'kd_tree', 'brute'),\n",
    "        }\n",
    "    }, \n",
    "    {\n",
    "        'model_name': 'GBM',\n",
    "        'model': GradientBoostingClassifier,\n",
    "        'params': {\n",
    "            'n_estimators': [100],\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        'model_name': 'Bayes',\n",
    "        'model': GaussianNB,\n",
    "        'params': {\n",
    "            'var_smoothing': [1e-7, 1e-8, 1e-9, 1e-10, 1e-11]\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        'model_name': 'RandomForest',\n",
    "        'model': RandomForestClassifier,\n",
    "        'params': {\n",
    "            'n_estimators': [100]\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        'model_name': 'SVM',\n",
    "        'model': SVC,\n",
    "        'params': {\n",
    "            'kernel': ('linear', 'rbf', 'poly'),\n",
    "            'gamma': [0.0001, 0.001, 0.01],\n",
    "            'C': [1, 3, 5, 7, 9]\n",
    "        }\n",
    "    }]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TestModels = pd.DataFrame()\n",
    "tmp = {}\n",
    "\n",
    "for pmodel in models_with_parameters:\n",
    "    model_name = pmodel['model_name']\n",
    "    tmp['Model'] = model_name\n",
    "    model = pmodel['model']()\n",
    "    parameters = pmodel['params']\n",
    "    \n",
    "    print(f\"Calculating {model_name} model\")\n",
    "    \n",
    "    print(\"Start training model...\")\n",
    "    if os.path.exists('ParamModels/{}_model.npy'.format(model_name)) and model_name != 'GBM' and model_name != 'RandomForest':\n",
    "        try:\n",
    "            model = np.load('ParamModels/{}_model.npy'.format(model_name), allow_pickle=True).item()\n",
    "        except ValueError:\n",
    "            model.fit(X2D_train, Y_train)\n",
    "            np.save(f\"ParamModels/{model_name}_model.npy\", model)\n",
    "            \n",
    "    else:\n",
    "        classifier = GridSearchCV(model, parameters, cv=2, iid=False, n_jobs=4)\n",
    "        classifier.fit(X2D_train, Y_train)\n",
    "        \n",
    "        grided_params = classifier.best_params_\n",
    "        \n",
    "        model.set_params(**grided_params)\n",
    "        model.fit(X2D_train, Y_train)\n",
    "        np.save(f\"ParamModels/{model_name}_model.npy\", model)\n",
    "    \n",
    "    print(\"Calculating scores....\")\n",
    "    tmp['Accuracy'] = accuracy_score(Y_test, model.predict(X2D_test))\n",
    "    tmp['Precision'] = precision_score(Y_test, model.predict(X2D_test), average='weighted')\n",
    "    tmp['Recall'] = recall_score(Y_test, model.predict(X2D_test), average='weighted')\n",
    "    tmp['F1_Score'] = f1_score(Y_test, model.predict(X2D_test), average='weighted')\n",
    "    \n",
    "    print(f\"Accuracy: {tmp['Accuracy']}\\nPrecision: {tmp['Precision']}\\nRecall: {tmp['Recall']}\\nF1 Score: {tmp['F1_Score']}\")\n",
    "    \n",
    "    TestModels = TestModels.append([tmp])\n",
    "\n",
    "TestModels.set_index('Model', inplace=True)\n",
    "fig, axes = plt.subplots(ncols=1, figsize=(10, 4))\n",
    "TestModels.Accuracy.plot(ax=axes, kind='bar', title='Accuracy')\n",
    "plt.show()\n",
    "\n",
    "#TestModels.set_index('Model', inplace=True)\n",
    "fig, axes = plt.subplots(ncols=1, figsize=(10, 4))\n",
    "TestModels.Precision.plot(ax=axes, kind='bar', title='Precision')\n",
    "plt.show()\n",
    "\n",
    "#TestModels.set_index('Model', inplace=True)\n",
    "fig, axes = plt.subplots(ncols=1, figsize=(10, 4))\n",
    "TestModels.Recall.plot(ax=axes, kind='bar', title='Recall')\n",
    "plt.show()\n",
    "\n",
    "#TestModels.set_index('Model', inplace=True)\n",
    "fig, axes = plt.subplots(ncols=1, figsize=(10, 4))\n",
    "TestModels.F1_Score.plot(ax=axes, kind='bar', title='F1_Score')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Optimization of hyper parameters gave little, the effectiveness of most models remained in place. None of the classical machine learning models matched the effectiveness of the simple neural network model but remained at a satisfactory level of about 95 f1 score."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
